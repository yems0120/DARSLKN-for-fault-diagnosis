{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# K折交叉验证\n",
    "def k_fold(data_X, data_y, name, device='cpu',\n",
    "           k=5, num_epochs=50, learning_rate=1e-3, weight_decay=0.0, batch_size=128):\n",
    "    train_l_sum, valid_l_sum = [], []\n",
    "    train_total_time = 0.0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, data_X, data_y) # X_train, y_train, X_valid, y_valid\n",
    "        net = get_net(device)\n",
    "        train_ls, valid_ls, train_time = train(name, i, net, *data, num_epochs, learning_rate, weight_decay, batch_size, device)\n",
    "        index = valid_ls.index(max(valid_ls)) # 返回列表最大值的索引\n",
    "        # train_l_sum += train_ls[index]\n",
    "        # valid_l_sum += valid_ls[index]\n",
    "        train_l_sum.append(train_ls[index])\n",
    "        valid_l_sum.append(valid_ls[index])\n",
    "        train_total_time += train_time\n",
    "        print(f'折{i + 1}, train_time(second): {train_time}' )\n",
    "        print(f'折{i + 1}, 训练log accuracy{float(train_ls[index]):f}, '\n",
    "              f'验证log accuracy{float(valid_ls[index]):f}')\n",
    "\n",
    "    return np.mean(train_l_sum), np.mean(valid_l_sum), np.std(valid_l_sum), train_total_time / k\n",
    "\n",
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "def build_dataloader(data_x, data_y, BATCH_SIZE=128, is_train=True):\n",
    "    # 构造一个Pytorch数据迭代器\n",
    "    data_tensor = TensorDataset(data_x, data_y)                    \n",
    "    data_loader = DataLoader(dataset=data_tensor, batch_size=BATCH_SIZE, shuffle=is_train)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练+验证\n",
    "def train(name, i, net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size, device):\n",
    "    # writer = SummaryWriter(log_dir=f'logs/{name}/折{i+1}')\n",
    "    # writer = {'train_loss': SummaryWriter(log_dir=f'logs/{name}/折{i+1}/train'),\n",
    "    #           'test_loss': SummaryWriter(log_dir=f'logs/{name}/折{i+1}/test')}\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = build_dataloader(train_features, train_labels, batch_size) # train_loader\n",
    "    test_iter = build_dataloader(test_features, test_labels, batch_size) # test_loader\n",
    "    # Adam优化算法\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr = learning_rate,\n",
    "                                 weight_decay = weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    total_time = 0.0\n",
    "    # max_correct = 0.0\n",
    "    BEST_LOSS = np.inf # 学习率衰减\n",
    "    patience_counter = 0    \n",
    "    for epoch in trange(1, num_epochs+1):\n",
    "        net.train()\n",
    "        start = time.time()\n",
    "        total_loss = 0.0\n",
    "        for ii, batch in enumerate(train_iter):\n",
    "            X, y = batch\n",
    "            X, y = X.unsqueeze(1).to(device=device), y.to(device=device)\n",
    "            pre = net(X)\n",
    "            loss = criterion(pre, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss +=loss.item()\n",
    "        \n",
    "        end = time.time()\n",
    "        time_elapsed = end - start\n",
    "        # writer.add_scalar(\"Time/train\", time_elapsed, epoch)\n",
    "        total_time += time_elapsed\n",
    "\n",
    "        net.eval()\n",
    "        for NAME, loader in [(\"train\", train_iter), (\"test\", test_iter)]:\n",
    "            loss_sum = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "            for x_input, y_label in loader:\n",
    "                x_input, y_label = x_input.unsqueeze(1).to(device=device), y_label.to(device=device)\n",
    "                y_pred = net(x_input)\n",
    "                LOSS = criterion(y_pred, y_label)\n",
    "                loss_sum += LOSS.item()\n",
    "                _, predicted = torch.max(y_pred, dim=1)\n",
    "\n",
    "                total += y_label.shape[0]\n",
    "                correct += int((predicted == y_label).sum())\n",
    "                \n",
    "            if NAME == 'train':\n",
    "                train_ls.append(correct/total)\n",
    "                # writer['train_loss'].add_scalar(\"Loss\", loss_sum/len(loader), epoch)\n",
    "                \n",
    "            else:\n",
    "                # 保存权重\n",
    "                # if correct > max_correct:\n",
    "                #     max_correct = correct\n",
    "                #     torch.save(net, f'{name}.pth')\n",
    "                    \n",
    "                test_ls.append(correct/total)\n",
    "                # writer['test_loss'].add_scalar(\"Loss\", loss_sum/len(loader), epoch)\n",
    "                \n",
    "                # 学习率衰减\n",
    "                if loss_sum < BEST_LOSS:\n",
    "                    BEST_LOSS = loss_sum\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= 5 :\n",
    "                        learning_rate = 0.2*learning_rate \n",
    "                        # print(learning_rate)\n",
    "                        optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                                    lr = learning_rate,\n",
    "                                                    weight_decay = weight_decay)\n",
    "                        patience_counter = 0 \n",
    "            # writer.add_scalar(name, correct/total, epoch)\n",
    "            \n",
    "    # writer['train_loss'].close()\n",
    "    # writer['test_loss'].close()   \n",
    "    return train_ls, test_ls, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "from modules.data_processing import *\n",
    "# from modules.setup_seed import *\n",
    "# from modules.cross_validation import k_fold\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "data_x = np.load('./data/Bearing_Zhaomh/SNR-N5/data_x.npy')\n",
    "data_y = np.load('./data/Bearing_Zhaomh/SNR-N5/data_y.npy')\n",
    "data_x, data_y = data_processing(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# __all__ = ('CBAM', 'SENet', 'autopad', 'Conv', 'Bottleneck, 'C2f', 'ARNet')\n",
    "\n",
    "# CBAM 注意力\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet.\"\"\"\n",
    "    def __init__(self, channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Conv1d(channels, channels, 1, 1, 0, bias=True)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self.act(self.fc(self.pool(x)))\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial-attention module.\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        \"\"\"Initialize Spatial-attention module with kernel size argument.\"\"\"\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.cv1 = nn.Conv1d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply channel and spatial attention on input for feature recalibration.\"\"\"\n",
    "        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1))) \n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module.\"\"\"\n",
    "    def __init__(self, c1, kernel_size=7):  # ch_in, kernels\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(c1)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the forward pass through C1 module.\"\"\"\n",
    "        return self.spatial_attention(self.channel_attention(x))\n",
    "# CBAM 注意力\n",
    "\n",
    "\n",
    "# SE 注意力\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super(SENet, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//ratio, False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels//ratio, channels, False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, c, L = x.size()\n",
    "        avg = self.avg_pool(x).view([N, c])\n",
    "        fc = self.fc(avg).view([N, c, 1])\n",
    "\n",
    "        return x*fc\n",
    "# SE 注意力\n",
    "\n",
    "\n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "    default_act = nn.SiLU() # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "    \n",
    "    \n",
    "class Bottleneck(nn.Module):  \n",
    "    \"\"\"DarknetBottleneck\"\"\"\n",
    "    def __init__(self, c1, c2, shortcut=True, e=0.5,\n",
    "                 k1=19, d1=2, k2=17, d2=4):\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k=k1, p=autopad(k=k1, d=d1), d=d1)\n",
    "        self.cv2 = Conv(c_, c2, k=k2, p=autopad(k=k2, d=d2), d=d2)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "    \n",
    "\n",
    "class C2f(nn.Module):  # CSPLayer_2Conv\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, e=0.5,\n",
    "                 k1=19, d1=2, k2=17, d2=4):\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, e, k1, d1, k2, d2) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))   \n",
    "        y.extend(m(y[-1]) for m in self.m)  \n",
    "        return self.cv2(torch.cat(y, 1))  # Concat -> ConvModule\n",
    "    \n",
    "class ARNet(nn.Module): # Attention_ReshuffleNet\n",
    "    def __init__(self, channels, ratio=1):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//ratio, False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(channels//ratio, channels, False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.size()\n",
    "        avg = self.avg_pool(x).view([B, C])\n",
    "        fc = self.fc(avg).view([B, C, 1])\n",
    "        x = x*fc\n",
    "        indices = torch.argsort(fc, dim=1, descending=False).expand(B, C, L)\n",
    "        x = torch.gather(x, dim=1, index=indices)\n",
    "        return x\n",
    "    \n",
    "class dilated_c2f1(nn.Module):\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, e=1,\n",
    "                 k1=19, d1=2, k2=17, d2=4):\n",
    "        super().__init__()\n",
    "        self.c = c1//2\n",
    "        self.ar = ARNet(channels=c1)        \n",
    "        self.cv = Conv((2 + n) * self.c, c2, 1)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, e, k1, d1, k2, d2) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.ar(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)  \n",
    "        return self.cv(torch.cat(y, 1))\n",
    "    \n",
    "    \n",
    "class dilated_c2f2(nn.Module):\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, e=1,\n",
    "                 k1=19, d1=2, k2=17, d2=4):\n",
    "        super().__init__()\n",
    "        self.c = c1//3\n",
    "        self.ar1 = ARNet(channels=c1)\n",
    "        self.ar2 = ARNet(channels=(c1-self.c))         \n",
    "        self.cv = Conv((5 + n) * self.c, c2, 1)\n",
    "        self.m1 = nn.ModuleList(Bottleneck(2*self.c, 2*self.c, shortcut, e, k1, d1, k2, d2) for _ in range(1))\n",
    "        self.m2 = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, e, k1, d1, k2, d2) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(torch.tensor_split(self.ar1(x), (self.c,), dim=1))\n",
    "        y.extend(m1(y[-1]) for m1 in self.m1)\n",
    "        x = self.ar2(y[-1])\n",
    "        y.pop()\n",
    "        y.extend(x.chunk(2, 1))\n",
    "        y.extend(m2(y[-1]) for m2 in self.m2)\n",
    "        return self.cv(torch.cat(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:11<00:00,  6.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1, train_time(second): 301.071964263916\n",
      "折1, 训练log accuracy0.999375, 验证log accuracy0.940000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:10<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折2, train_time(second): 304.9759786128998\n",
      "折2, 训练log accuracy1.000000, 验证log accuracy0.940000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:12<00:00,  6.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折3, train_time(second): 303.623033285141\n",
      "折3, 训练log accuracy0.996875, 验证log accuracy0.940000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:16<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折4, train_time(second): 305.4885232448578\n",
      "折4, 训练log accuracy0.996875, 验证log accuracy0.921250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:05<00:00,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折5, train_time(second): 295.38654804229736\n",
      "折5, 训练log accuracy0.999062, 验证log accuracy0.941250\n",
      "valid_a 0.9365\n",
      "valid_s 0.007640353394968046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    c = 48\n",
    "    self.conv0 = Conv(1, c, k=1, s=1)\n",
    "\n",
    "    self.conv1 = Conv(c, 2*c, k=3, s=2, p=1)\n",
    "    self.c2f1 = dilated_c2f1(2*c, 2*c, n=1, shortcut=True, e=1,\n",
    "                             k1=39, d1=2, k2=35, d2=5)\n",
    "    \n",
    "    self.conv2 = Conv(2*c, 3*c, k=3, s=2, p=1)\n",
    "    self.c2f2 = dilated_c2f2(3*c, 3*c, n=1, shortcut=True, e=1,\n",
    "                             k1=29, d1=2, k2=27, d2=5)\n",
    "    \n",
    "    self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    self.drop = nn.Dropout(p=0.)\n",
    "    self.fc = nn.Linear(3*c, 5)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # x.shape: (batch, 1024, 1)\n",
    "    x = self.conv0(x)\n",
    "    \n",
    "    x = self.conv1(x)\n",
    "    x = self.c2f1(x)\n",
    "    \n",
    "    x = self.conv2(x)\n",
    "    x = self.c2f2(x)\n",
    "    \n",
    "    x = self.pool(x).flatten(1)\n",
    "    x = self.drop(x)\n",
    "    x = self.fc(x) \n",
    "    return x\n",
    "\n",
    "def get_net(device):\n",
    "    net = Model().to(device=device)\n",
    "    return net\n",
    "\n",
    "train_a, valid_a, valid_s, average_time = k_fold(data_x, data_y, 'Model', device,\n",
    "                        k=5, num_epochs=70, learning_rate=0.001)\n",
    "# print('train_a', train_a)\n",
    "print('valid_a', valid_a)\n",
    "print('valid_s', valid_s)\n",
    "# print('time', average_time)    \n",
    "# 升序   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:10<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1, train_time(second): 296.68719005584717\n",
      "折1, 训练log accuracy0.995313, 验证log accuracy0.928750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:05<00:00,  6.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折2, train_time(second): 292.10922932624817\n",
      "折2, 训练log accuracy0.975000, 验证log accuracy0.930000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:16<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折3, train_time(second): 300.46922159194946\n",
      "折3, 训练log accuracy0.897188, 验证log accuracy0.897500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:24<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折4, train_time(second): 301.6461844444275\n",
      "折4, 训练log accuracy0.986563, 验证log accuracy0.921250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:07<00:00,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折5, train_time(second): 292.6357831954956\n",
      "折5, 训练log accuracy0.920312, 验证log accuracy0.912500\n",
      "valid_a 0.9179999999999999\n",
      "valid_s 0.012005207203542988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ARNet(nn.Module): # Attention_ReshuffleNet\n",
    "    def __init__(self, channels, ratio=1):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//ratio, False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(channels//ratio, channels, False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.size()\n",
    "        avg = self.avg_pool(x).view([B, C])\n",
    "        fc = self.fc(avg).view([B, C, 1])\n",
    "        x = x*fc\n",
    "        indices = torch.argsort(fc, dim=1, descending=True).expand(B, C, L)\n",
    "        x = torch.gather(x, dim=1, index=indices)\n",
    "        return x\n",
    "    \n",
    "train_a, valid_a, valid_s, average_time = k_fold(data_x, data_y, 'Model', device,\n",
    "                        k=5, num_epochs=70, learning_rate=0.001)\n",
    "# print('train_a', train_a)\n",
    "print('valid_a', valid_a)\n",
    "print('valid_s', valid_s)\n",
    "# print('time', average_time)  \n",
    "# 降序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [07:08<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1, train_time(second): 300.936137676239\n",
      "折1, 训练log accuracy1.000000, 验证log accuracy0.922500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:45<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折2, train_time(second): 285.88668489456177\n",
      "折2, 训练log accuracy1.000000, 验证log accuracy0.933750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:51<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折3, train_time(second): 289.5573847293854\n",
      "折3, 训练log accuracy1.000000, 验证log accuracy0.925000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:56<00:00,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折4, train_time(second): 293.63865399360657\n",
      "折4, 训练log accuracy1.000000, 验证log accuracy0.932500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:56<00:00,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折5, train_time(second): 294.43532633781433\n",
      "折5, 训练log accuracy1.000000, 验证log accuracy0.945000\n",
      "valid_a 0.9317500000000001\n",
      "valid_s 0.007889866919029728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ARNet(nn.Module): # Attention_ReshuffleNet\n",
    "    def __init__(self, channels, ratio=1):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//ratio, False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(channels//ratio, channels, False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.size()\n",
    "        avg = self.avg_pool(x).view([B, C])\n",
    "        fc = self.fc(avg).view([B, C, 1])\n",
    "        x = x*fc\n",
    "        # indices = torch.argsort(fc, dim=1, descending=True).expand(B, C, L)\n",
    "        # x = torch.gather(x, dim=1, index=indices)\n",
    "        return x\n",
    "    \n",
    "train_a, valid_a, valid_s, average_time = k_fold(data_x, data_y, 'Model', device,\n",
    "                        k=5, num_epochs=70, learning_rate=0.001)\n",
    "# print('train_a', train_a)\n",
    "print('valid_a', valid_a)\n",
    "print('valid_s', valid_s)\n",
    "# print('time', average_time)  \n",
    "# 无排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:43<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1, train_time(second): 280.00688123703003\n",
      "折1, 训练log accuracy0.994062, 验证log accuracy0.918750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:45<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折2, train_time(second): 283.43673157691956\n",
      "折2, 训练log accuracy0.997812, 验证log accuracy0.927500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:57<00:00,  5.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折3, train_time(second): 292.08569717407227\n",
      "折3, 训练log accuracy0.996563, 验证log accuracy0.923750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:41<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折4, train_time(second): 279.4677209854126\n",
      "折4, 训练log accuracy0.987812, 验证log accuracy0.930000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:42<00:00,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折5, train_time(second): 274.9848575592041\n",
      "折5, 训练log accuracy0.965625, 验证log accuracy0.937500\n",
      "valid_a 0.9275\n",
      "valid_s 0.006274950199005588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ARNet(nn.Module): # Attention_ReshuffleNet\n",
    "    def __init__(self, channels, ratio=1):\n",
    "        super(ARNet, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//ratio, False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(channels//ratio, channels, False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.size()\n",
    "        avg = self.avg_pool(x).view([B, C])\n",
    "        fc = self.fc(avg).view([B, C, 1])\n",
    "        # x = x*fc\n",
    "        indices = torch.argsort(fc, dim=1, descending=True).expand(B, C, L)\n",
    "        x = torch.gather(x, dim=1, index=indices)\n",
    "        return x\n",
    "    \n",
    "train_a, valid_a, valid_s, average_time = k_fold(data_x, data_y, 'Model', device,\n",
    "                        k=5, num_epochs=70, learning_rate=0.001)\n",
    "# print('train_a', train_a)\n",
    "print('valid_a', valid_a)\n",
    "print('valid_s', valid_s)\n",
    "# print('time', average_time)  \n",
    "# 无强化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:26<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1, train_time(second): 273.7907338142395\n",
      "折1, 训练log accuracy1.000000, 验证log accuracy0.916250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:31<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折2, train_time(second): 275.6815013885498\n",
      "折2, 训练log accuracy1.000000, 验证log accuracy0.925000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:31<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折3, train_time(second): 273.21705961227417\n",
      "折3, 训练log accuracy1.000000, 验证log accuracy0.922500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:28<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折4, train_time(second): 274.1193013191223\n",
      "折4, 训练log accuracy1.000000, 验证log accuracy0.928750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:54<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折5, train_time(second): 288.4284772872925\n",
      "折5, 训练log accuracy1.000000, 验证log accuracy0.936250\n",
      "valid_a 0.9257500000000001\n",
      "valid_s 0.006642665127793214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ARNet(nn.Module): # Attention_ReshuffleNet\n",
    "    def __init__(self, channels, ratio=1):\n",
    "        super(ARNet, self).__init__()\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(channels, channels//ratio, False),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(channels//ratio, channels, False),\n",
    "        #     nn.Sigmoid(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, C, L = x.size()\n",
    "        # avg = self.avg_pool(x).view([B, C])\n",
    "        # fc = self.fc(avg).view([B, C, 1])\n",
    "        # x = x*fc\n",
    "        # indices = torch.argsort(fc, dim=1, descending=True).expand(B, C, L)\n",
    "        # x = torch.gather(x, dim=1, index=indices)\n",
    "        return x\n",
    "    \n",
    "train_a, valid_a, valid_s, average_time = k_fold(data_x, data_y, 'Model', device,\n",
    "                        k=5, num_epochs=70, learning_rate=0.001)\n",
    "# print('train_a', train_a)\n",
    "print('valid_a', valid_a)\n",
    "print('valid_s', valid_s)\n",
    "# print('time', average_time)  \n",
    "# 无注意"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
